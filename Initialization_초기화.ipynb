{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhQfSRg7BC1c4+SsQTBpZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sun-in-universe/blue_dot/blob/main/Initialization_%EC%B4%88%EA%B8%B0%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEpLFOv5iQS-"
      },
      "outputs": [],
      "source": [
        "- 초기 가중치 지정 : 가중치를 초기화하는 것이 학습 과정을 도움\n",
        "- random, zeros, Xavier initialization\n",
        "\n",
        "* 기울기 하강 수렴을 빠르게 한다.\n",
        "* 경사 하강법이 더 낮은 훈련 (및 일반화) 오차로 수렴할 확률을 높이다\n",
        "= 경사하강법을 빠르게, 적은 오차로 만드는 것이 좋은 Initialization !"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<1> wegiht = zero initialization\n",
        "- 모든 weight = 0, 모든 결론이 zero가 된다.\n",
        "\n",
        "결론 !\n",
        "(1) 가중치를 랜덤하게 설정해서 대칭성을 없애야한다.\n",
        "(2) b를 0으로 초기화하는 것은 괜찮다.\n",
        "\n",
        "<2> weight = random initializaiton\n",
        "- large random-valued weight -> 손실을 높임\n",
        "- log(AL) = log(0)이면 손실은 무한대로 증폭\n",
        "- 초기화를 잘못 설정하면 기울기 소실 or 폭발 문제가 일어나고-> 알고리즘의 최적화를 방해한다.\n",
        "- 오랫동안 훈련을 시키면 더 나은 결과를 낼 수 있긴 하지만, 매우 큰 random 수로 초기화하는 것은 최적화를 느리게 한다.\n",
        "\n",
        "결론 !\n",
        "(1) very large random values로 초기화 하면 안된다.\n",
        "(2) small rnadom values가 낫지만, 얼마나 작아야 하냐는 여전히 의문점으로 남는다.\n",
        "\n",
        "<3> weight = Xavier initialization\n",
        "- sqrt(2./layers_dims[l-1])\n",
        "\n"
      ],
      "metadata": {
        "id": "MICBJPES0puy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize_Xavier_parameter\n",
        "\n",
        "def initialize_Xavier_parameter(layer_dims):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  layer_dims -- python array contining the dimensions of each layer in our network ex) [2, 5, 4... ]\n",
        "\n",
        "  Returns:\n",
        "  parameters -- python dictionary containing your parmaeters \"W1\", \"b1\"... \"WL\", \"bL\":\n",
        "  Wl = weight matrix of shape(layer_dims[l], layer_dims[l-1])\n",
        "  b1 = bias vector of shape(layer_dims[l], 1)\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(3)\n",
        "  parameters = {}\n",
        "  L = len(layer_dims)\n",
        "\n",
        "  for i in range(1, L):\n",
        "    parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * np.sqrt(2. / layers_dims[l-1])\n",
        "    parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "Can2BqRtpVDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward propagation\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  A--activations from previous layer(or input data)\n",
        "  W--weights matrix: numpy array of shape\n",
        "  b--bias vector\n",
        "\n",
        "  Returns:\n",
        "  Z -- the input of the activation function, also called pre-activation parameter\n",
        "  cache -- a python tuple containing \"A\", \"W\", \"b\"\n",
        "  \"\"\"\n",
        "\n",
        "  Z = np.dot(W, A) + b\n",
        "  cache = (A, W, b)\n",
        "\n",
        "  return Z, cache\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "  cache = (A, W, b)\n",
        "  return 1 / (1 + np.exp(-z)), cache\n",
        "\n",
        "def relu(z):\n",
        "  cache = (A, W, b)\n",
        "  return max(0, z), cache\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = sigmoid(Z)\n",
        "  elif activation == \"relu\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = relu(Z)\n",
        "\n",
        "  return A, cache\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "  \"\"\"\n",
        "  implement forward propagation for the [Linear->Relu]*(L-1)->Linear->Sigmoid computation\n",
        "\n",
        "  Arguments:\n",
        "  X -- data\n",
        "  parameters -- out of initialize_parameters_deep()\n",
        "\n",
        "  Returns:\n",
        "  AL -- activation value from the output(last) layer\n",
        "  caches -list of caches containing\n",
        "  \"\"\"\n",
        "\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters) // 2\n",
        "\n",
        "  for i in range(1, L):\n",
        "    A_prev = A\n",
        "    A, cache = linear_activation_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], \"relu\")\n",
        "    caches.append(caches)\n",
        "\n",
        "  AL, caches = linear_activation_forward(A, parameters[f'W{l}'], parameters[f'b{l}'], \"sigmoid\")\n",
        "\n",
        "  return AL, caches"
      ],
      "metadata": {
        "id": "3U8Y-olvlysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute cost\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "  \"\"\"\n",
        "\n",
        "  Arguments:\n",
        "  AL -- probability vector corresponding to your label predictions, shape(1, numeber of examples)\n",
        "  Y -- true \"\"label\"\" vector, shape(1, numer of examples)\n",
        "\n",
        "  Returns:\n",
        "  cost -- corss-entropy cost\n",
        "  \"\"\"\n",
        "\n",
        "  m = Y.shape[1] #Y의 갯수\n",
        "\n",
        "  cost = -(1/m) * np.sum((Y * np.log(AL) + ((1-Y) * np.log(1-AL))))\n",
        "  dZ = AL - Y  # This is the derivative of the cost with respect to Z\n",
        "\n",
        "  cost = np.squeeze(cost)\n",
        "\n",
        "  return cost, dZ"
      ],
      "metadata": {
        "id": "pZ_YDzTyIvWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward propagation\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  dZ -- Gradient of the cost with respect to the linear output\n",
        "  cache -- tuple of values(A_prev, W, b) coming from the forward propagation (A, W, b)\n",
        "\n",
        "  Returns:\n",
        "  dA_prev -- Gradient of the cost with respect to the activation, same shape as A_prev\n",
        "  dW -- Gradient of the cost with respect to W\n",
        "  db -- Gradient of the cost with respoect to b\n",
        "  \"\"\"\"\n",
        "\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1]\n",
        "\n",
        "  dw = (1/m) * np.dot(dZ, A_prev.T)\n",
        "  db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "\n",
        "def relu_backward(dA, activation_cache):\n",
        "    Z = activation_cache\n",
        "    dZ = np.array(dA, copy=True)  # Initialize dZ with dA, then set dZ=0 where Z<=0\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, activation_cache):\n",
        "    Z = activation_cache\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    dZ = dA * A * (1 - A)\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  dA -- post activation gradient for current layer\n",
        "  cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "  activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "  Returns:\n",
        "  dA_prev -- Gradient of the cost with respect to the activation\n",
        "  dW -- Gradient of the cost with respect to W\n",
        "  db -- Gradient of the cost with respoect to b\n",
        "  \"\"\"\n",
        "\n",
        "  linear_cache, activation_cache = cache\n",
        "\n",
        "  if activation == \"relu\":\n",
        "    dZ = relu_backward(dA, activation_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation == \"sigmoid\":\n",
        "    dZ = sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "\n",
        "def backward_propagation(AL, Y, caches):\n",
        "  \"\"\"\n",
        "  implement the backward propagation for the [Linear->Relu] * (L-1) -> Linear -> Sigmoid group\n",
        "\n",
        "  Arguments:\n",
        "  AL -- probability vector, out of the forward propagation\n",
        "  Y-- true \"label\" vector\n",
        "  caches -- list of caches containing => every caches of linear_activation_forward with \"relu\" and \"sigmoid\"\n",
        "\n",
        "  Returns:\n",
        "  grads -- A dictionary with the gradients\n",
        "           grads[\"dA\" + str(l)] =\n",
        "           grads[\"dW\" + str(l)] =\n",
        "           grads[\"db\" + str(l)] =\n",
        "  \"\"\"\n",
        "\n",
        "  grads = {}\n",
        "  L = len(caches)\n",
        "  m = AL.shape[1]\n",
        "  Y = Y.reshape(AL.shape)\n",
        "\n",
        "  #initializaing the backpropagation\n",
        "  dAL = -(np.divdie(Y, AL)-np.divde(1-Y, 1-AL))\n",
        "\n",
        "  #Lth layer(sigmoid->Linear) gradients. Inputs: dAL, current cache, Outputs: grads[\"dAL-1\"], grads[\"dWL\"], grad[\"dbL\"]\n",
        "  current_cache = caches[L-1]\n",
        "  dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "  grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "  grads[\"dW\" + str(L)] = dW_temp\n",
        "  grads[\"db\" + str(L)] = db_temp\n",
        "\n",
        "  for l in reversed(range(L-1)):\n",
        "    current_cache = caches[l]\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
        "    grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "    grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "  return grads\n"
      ],
      "metadata": {
        "id": "zHjKIBNqJyNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "current_cache = caches[L-1]: 이 줄은 캐시 리스트(caches)에서 마지막 층(L번째 층)의 캐시된 값들 (linear_cache 및 activation_cache)을 current_cache에 초기화합니다.\n",
        "\n",
        "dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\"): 이 줄은 마지막 (L번째) 층에 대한 기울기를 계산합니다. linear_activation_backward 함수를 사용하며 다음과 같은 인자들을 사용합니다:\n",
        "\n",
        "dAL은 현재 (L번째) 층의 활성화에 대한 비용의 기울기입니다.\n",
        "current_cache는 현재 층에 대한 캐시된 값들을 포함합니다.\n",
        "\"sigmoid\"는 이 층에서 시그모이드 활성화 함수를 사용함을 나타냅니다.\n",
        "계산된 기울기들 (dA_prev_temp, dW_temp, db_temp)은 그런 다음 적절한 키로 grads 사전에 저장됩니다. 이는 마지막 층에 대한 기울기를 나타냅니다.\n",
        "\n",
        "코드는 L-1번째부터 역방향으로 순회하는 루프에 들어갑니다. 이때 L은 네트워크의 층 수를 나타냅니다.\n",
        "\n",
        "current_cache = caches[l]: 이 줄은 현재 층 l에 대한 캐시된 값들을 current_cache에 초기화합니다.\n",
        "\n",
        "dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\"): 이 줄은 현재 층 l에 대한 기울기를 계산합니다. linear_activation_backward 함수를 사용하며 다음과 같은 인자들을 사용합니다:\n",
        "\n",
        "grads[\"dA\" + str(l + 1)]은 현재 (l+1)번째 층의 활성화에 대한 비용의 기울기입니다. 이전 층에 대한 기울기입니다.\n",
        "current_cache는 현재 층에 대한 캐시된 값들을 포함합니다.\n",
        "\"relu\"는 이 층에서 ReLU 활성화 함수를 사용함을 나타냅니다.\n",
        "계산된 기울기들 (dA_prev_temp, dW_temp, db_temp)은 그런 다음 적절한 키로 grads 사전에 저장됩니다. 이는 현재 층 l에 대한 기울기를 나타냅니다."
      ],
      "metadata": {
        "id": "M202rprSfmBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  parameters -- python dictionary containing your parameters\n",
        "  grads -- also dict containing gradients, output of backpropagation\n",
        "\n",
        "  Returns:\n",
        "  parameters -- also dict containing your updated parameters\n",
        "\n",
        "  \"\"\"\n",
        "  parameters = parameters.copy()\n",
        "  L = len(parameters) // 2\n",
        "\n",
        "  for l in range(L):\n",
        "    parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
        "    parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
        "\n",
        "  return parameters\n"
      ],
      "metadata": {
        "id": "Vf9Bn2fglXmK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 모델 만들기\n",
        "2. 라이브러리 임포트\n",
        "3. numpy 임의성 조정 with random_seed\n",
        "4. 데이터 셋 가져오기\n",
        "5. train, val, test set으로 나누기\n",
        "6. initializaing parameters\n",
        "7. forward propagation\n",
        "8. cost computation\n",
        "9. parameters update\n",
        "10. train_nn 모델 최종 만들기 및 테스트"
      ],
      "metadata": {
        "id": "q8qbl82KlM4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망을 학습시키는 함수\n",
        "def train_nn(X_train, Y_train, X_test, Y_test, neurons_per_layer, epoch, alpha):\n",
        "  parameters = initialize_Xavier_parameter(neurons_per_layer)\n",
        "  lost_list = []\n",
        "  m = X_train.shape[0]\n",
        "\n",
        "  #epoch번 경사하강을 한다\n",
        "  for i in range(epoch):\n",
        "    parameters.copy = parameters.copy()\n",
        "\n",
        "    for x, y, in zip(X_train, Y_train):\n",
        "      prediction, cache = forward_propagation(X, parameters)\n",
        "      grads = backward_propagation(AL, Y, caches)\n",
        "      parameters_copy = update(parameters_copy, grads, learning_rate)\n",
        "\n",
        "      parameters = parameters_copy\n",
        "      loss_list.append(compute_loss(X_train, Y_train, parameters))\n",
        "\n",
        "  return loss_list, parameters"
      ],
      "metadata": {
        "id": "-l_-d8H4jX_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}