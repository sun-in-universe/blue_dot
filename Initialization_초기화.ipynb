{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbYgEHgC3MUlysorDUE4wK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sun-in-universe/blue_dot/blob/main/Initialization_%EC%B4%88%EA%B8%B0%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEpLFOv5iQS-"
      },
      "outputs": [],
      "source": [
        "- 초기 가중치 지정 : 가중치를 초기화하는 것이 학습 과정을 도움\n",
        "- random, zeros, Xavier initialization\n",
        "\n",
        "* 기울기 하강 수렴을 빠르게 한다.\n",
        "* 경사 하강법이 더 낮은 훈련 (및 일반화) 오차로 수렴할 확률을 높이다\n",
        "= 경사하강법을 빠르게, 적은 오차로 만드는 것이 좋은 Initialization !"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<1> wegiht = zero initialization\n",
        "- 모든 weight = 0, 모든 결론이 zero가 된다.\n",
        "\n",
        "결론 !\n",
        "(1) 가중치를 랜덤하게 설정해서 대칭성을 없애야한다.\n",
        "(2) b를 0으로 초기화하는 것은 괜찮다.\n",
        "\n",
        "<2> weight = random initializaiton\n",
        "- large random-valued weight -> 손실을 높임\n",
        "- log(AL) = log(0)이면 손실은 무한대로 증폭\n",
        "- 초기화를 잘못 설정하면 기울기 소실 or 폭발 문제가 일어나고-> 알고리즘의 최적화를 방해한다.\n",
        "- 오랫동안 훈련을 시키면 더 나은 결과를 낼 수 있긴 하지만, 매우 큰 random 수로 초기화하는 것은 최적화를 느리게 한다.\n",
        "\n",
        "결론 !\n",
        "(1) very large random values로 초기화 하면 안된다.\n",
        "(2) small rnadom values가 낫지만, 얼마나 작아야 하냐는 여전히 의문점으로 남는다.\n",
        "\n",
        "<3> weight = Xavier initialization\n",
        "- sqrt(2./layers_dims[l-1])\n",
        "\n"
      ],
      "metadata": {
        "id": "MICBJPES0puy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize_Xavier_parameter\n",
        "\n",
        "def initialize_Xavier_parameter(layer_dims):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  layer_dims -- python array contining the dimensions of each layer in our network ex) [2, 5, 4... ]\n",
        "\n",
        "  Returns:\n",
        "  parameters -- python dictionary containing your parmaeters \"W1\", \"b1\"... \"WL\", \"bL\":\n",
        "  Wl = weight matrix of shape(layer_dims[l], layer_dims[l-1])\n",
        "  b1 = bias vector of shape(layer_dims[l], 1)\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(3)\n",
        "  parameters = {}\n",
        "  L = len(layer_dims)\n",
        "\n",
        "  for i in range(1, L):\n",
        "    parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * np.sqrt(2. / layers_dims[l-1])\n",
        "    parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "Can2BqRtpVDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward propagation\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  A--activations from previous layer(or input data)\n",
        "  W--weights matrix: numpy array of shape\n",
        "  b--bias vector\n",
        "\n",
        "  Returns:\n",
        "  Z -- the input of the activation function, also called pre-activation parameter\n",
        "  cache -- a python tuple containing \"A\", \"W\", \"b\"\n",
        "  \"\"\"\n",
        "\n",
        "  Z = np.dot(W, A) + b\n",
        "  cache = (A, W, b)\n",
        "\n",
        "  return Z, cache\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "  cache = (A, W, b)\n",
        "  return 1 / (1 + np.exp(-z)), cache\n",
        "\n",
        "def relu(z):\n",
        "  cache = (A, W, b)\n",
        "  return max(0, z), cache\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = sigmoid(Z)\n",
        "  elif activation == \"relu\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = relu(Z)\n",
        "\n",
        "  return A, cache\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "  \"\"\"\n",
        "  implement forward propagation for the [Linear->Relu]*(L-1)->Linear->Sigmoid computation\n",
        "\n",
        "  Arguments:\n",
        "  X -- data\n",
        "  parameters -- out of initialize_parameters_deep()\n",
        "\n",
        "  Returns:\n",
        "  AL -- activation value from the output(last) layer\n",
        "  caches -list of caches containing\n",
        "  \"\"\"\n",
        "\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters) // 2\n",
        "\n",
        "  for i in range(1, L):\n",
        "    A_prev = A\n",
        "    A, cache = linear_activation_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], \"relu\")\n",
        "    caches.append(caches)\n",
        "\n",
        "  AL, caches = linear_activation_forward(A, parameters[f'W{l}'], parameters[f'b{l}'], \"sigmoid\")\n",
        "\n",
        "  return AL, caches"
      ],
      "metadata": {
        "id": "3U8Y-olvlysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network model\n",
        "\n",
        "def model(X, Y, learning_rate = 0.01, num_iteration = 15000, initialization=\"Xavier\" ):\n",
        "  \"\"\"\n",
        "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (2, number of examples)\n",
        "    X.shape[1] is the number of examples (rows)\n",
        "    X.shape[0] is the number of features or attributes (columns)\n",
        "\n",
        "    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n",
        "    learning_rate -- hyper parameters in learning rate for gradient descent\n",
        "    num_iterations -- number of iterations to run gradient descent\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    initialization -- flag to choose which initialization to use (\"Xavier\")\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model\n",
        "    \"\"\"\n",
        "\n",
        "    grads = {}\n",
        "    costs = [] # to keep track of the loss\n",
        "    m = X.shape[1] # number of examples\n",
        "    layer_dims = [X.shape[0], 10, 5, 1]\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "      # Forward propagation : Linear > Relu > Linear > Relu >  Linear > Sigmoid\n",
        "      AL, caches = forward_propagation(X, parameters)"
      ],
      "metadata": {
        "id": "-l_-d8H4jX_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fD35eFo_iSpw"
      }
    }
  ]
}