{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWA7Yg7zFibcKFF7DmArtr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sun-in-universe/blue_dot/blob/main/IMDB_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%84%B8%ED%8A%B8%EC%97%90_RNN_%EC%A0%81%EC%9A%A9%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기초 RNN\n",
        "\n",
        "1. 훈련 세트와 검증세트 준비하기\n",
        "2. 샘플 길이 맞추고 원-핫 인코딩 하기\n",
        "3. SImpleRNN 클래스로 RNN 모델 만들기\n",
        "4. RNN 모델 훈련하기\n",
        "5. 임베딩층으로 RNN 모델 훈련하기"
      ],
      "metadata": {
        "id": "t-Ofnw8FF5tr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt3rdzT-FiUx",
        "outputId": "177862a1-4764-49d4-8ef7-74d99c1cea09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#매개변수 1. skip_top : 건너뛸 단어 개수 지정 (a, the, is 등)\n",
        "#매개변수 2. num_words : 훈련에 사용할 단어 개수 지정\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "(x_train_all, y_train_all), (x_test, y_test) = imdb.load_data(skip_top=20, num_words=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련 세트의 샘플 확인하기-영단어를 고유한 정수에 일대일 대응\n",
        "\n",
        "print(x_train_all[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO37FLQ_F37i",
        "outputId": "22353317-775a-4817-a19e-23f53e8598f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2, 22, 2, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 2, 2, 36, 2, 2, 25, 2, 43, 2, 2, 50, 2, 2, 2, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 39, 2, 2, 2, 2, 2, 2, 38, 2, 2, 2, 2, 50, 2, 2, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 22, 71, 87, 2, 2, 43, 2, 38, 76, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 2, 2, 2, 2, 62, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 66, 2, 33, 2, 2, 2, 2, 38, 2, 2, 25, 2, 51, 36, 2, 48, 25, 2, 33, 2, 22, 2, 2, 28, 77, 52, 2, 2, 2, 2, 82, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 2, 2, 2, 2, 2, 2, 88, 2, 2, 2, 2, 98, 32, 2, 56, 26, 2, 2, 2, 2, 2, 2, 2, 22, 21, 2, 2, 26, 2, 2, 2, 30, 2, 2, 51, 36, 28, 2, 92, 25, 2, 2, 2, 65, 2, 38, 2, 88, 2, 2, 2, 2, 2, 2, 2, 2, 32, 2, 2, 2, 2, 2, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련 세트에서 0, 1, 2 제거하기 -> 0=패딩, 1=글의 시작, 2=어휘사전에 없는 단어\n",
        "#패딩은 입력 시퀀스의 길이를 맞추기 위해, 길이가 짧은 시퀀스에 임의의 값을 추가하는 작업을 의미합니다. 이를 통해 모든 입력 데이터가 동일한 길이를 갖게 되어 모델에 쉽게 입력할 수 있습니다.\n",
        "\n",
        "for i in range(len(x_train_all)):\n",
        "  x_train_all[i] = [w for w in x_train_all[i] if w > 2]\n",
        "\n",
        "  print(x_train_all[0])"
      ],
      "metadata": {
        "id": "yCrm1RVXF4MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘사전 내려받기\n",
        "# 훈련세트 -> 영단어 변경위해서는 어휘사전 필요 : get_word_index() 사용\n",
        "\n",
        "word_to_index = imdb.get_word_index()\n",
        "word_to_index['movie']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9hW7hMvJQ2D",
        "outputId": "16d6a1cc-6c2d-498b-dd5d-438eced58034"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#총 88584만큼의 영단어\n",
        "len(word_to_index.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFzX5xxYJ4ah",
        "outputId": "26748dd8-fa32-4e8e-9b35-55fc64d900e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88584"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {word: index for index, word in word_to_index.items()}\n",
        "\n",
        "for w in x_train_all[0]:\n",
        "  print(index_to_word[w - 3], end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMK7a4YySaIM",
        "outputId": "568dbeb4-ecf0-4840-fdf8-1cf862af4593"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "film just story really they you just there an from so there film film were great just so much film would really at so you what they if you at film have been good also they were just are out because them all up are film but are be what they have don't you story so because all all "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련세트에 있는 정수는 3이상부터 영단어를 의미-> index_to_word에서 0, 1, 2를 지워줘야 한다.\n",
        "\n",
        "# index_to_word = {word_to_index[k] for k in word_to_index}\n",
        "\n",
        "# for w in x_train_all[0]:\n",
        "#   print(index_to_word[w - 3], end=' ')"
      ],
      "metadata": {
        "id": "ydJe9I-cJ9Zj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련세트의 입력데이터는 numpy배열이 아님 !\n",
        "\n",
        "print(len(x_train_all[0]), len(x_train_all[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JyAd7lhRrkK",
        "outputId": "34cb8c5b-ddc1-4b56-9596-fa56b18129d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 타깃 데이터는 0(부정), 1(긍정)으로 나타남\n",
        "\n",
        "print(y_train_all[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS5DZE_cTONp",
        "outputId": "5bdd99e8-e948-4000-c02e-e4faa3bb9f05"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0 0 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 25000개 훈련 세트 중 5000개만 분리하여 검증 세트로 사용\n",
        "# permutation 함수 = 주어진 숫자의 모든 순열 만들기 1 2 3, 2, 1, 3 등등... -> 데이터 섞기\n",
        "\n",
        "np.random.seed(42)\n",
        "random_index = np.random.permutation(25000)\n",
        "\n",
        "x_train = x_train_all[random_index[:20000]]\n",
        "y_train = x_train_all[random_index[:20000]]\n",
        "\n",
        "x_val = x_train_all[random_index[20000:]]\n",
        "y_val = x_train_all[random_index[20000:]]"
      ],
      "metadata": {
        "id": "VtHF451QTbNg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 길이 맞추기 -> pad_sequence() 함수 사용\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "maxlen = 100\n",
        "x_train_seq = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val_seq = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "\n",
        "# 각 샘플 길이가 100으로 변경 확인\n",
        "\n",
        "print(x_train_seq.shape, x_val_seq.shape)\n",
        "\n",
        "# 첫번째 샘플의 왼쪽이 0으로 채워짐\n",
        "\n",
        "print(x_train_seq[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjDZ-sC3UtgG",
        "outputId": "c6984520-c701-4082-dbd7-0f0f88015760"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100) (5000, 100)\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 35 40 27 28 40 22 83 31 85 45\n",
            " 24 23 31 70 31 76 30 98 32 22 28 51 75 56 30 33 97 53 38 46 53 74 31 35\n",
            " 23 34 22 58]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<RNN에서 원핫 인코딩 하는 이유>\n",
        "\n",
        "RNN(Recurrent Neural Network)은 순차적인 데이터를 다루기 위해 사용되는 딥 러닝 모델입니다. RNN은 입력 데이터의 순서를 고려하여 처리하는데, 이러한 특성 때문에 입력 데이터를 적절한 형태로 변환해야 합니다. 원핫 인코딩은 RNN에서 입력 데이터를 처리하기 위한 중요한 전처리 기법 중 하나입니다.\n",
        "\n",
        "원핫 인코딩은 카테고리 형태의 데이터를 컴퓨터가 이해하기 쉽도록 변환하는 방법 중 하나입니다. 특히 텍스트 데이터와 같이 토큰들이 특정 범주를 나타내는 경우에 많이 사용됩니다. 원핫 인코딩은 다음과 같은 과정을 거칩니다:\n",
        "\n",
        "데이터에 있는 모든 유니크한 토큰들에 고유한 정수 인덱스를 부여합니다.\n",
        "각 토큰에 해당하는 인덱스 위치에 1을 표시하고, 나머지 인덱스 위치에는 0을 표시하여 토큰을 벡터 형태로 표현합니다.\n",
        "예를 들어, 단어 \"apple\"을 원핫 인코딩으로 표현하면 [0, 0, 0, 1, 0]과 같이 됩니다. 여기서 [0, 0, 0, 1, 0]은 5차원 벡터로, 첫 번째 인덱스가 0이고 다른 인덱스들이 0이기 때문에 \"apple\"이 해당하는 인덱스 위치에만 1을 가지고 나머지는 0으로 표현됩니다.\n",
        "\n",
        "RNN에서 훈련 데이터를 원핫 인코딩하는 이유는 다음과 같습니다:\n",
        "\n",
        "이산적인 텍스트 데이터 표현: 텍스트 데이터는 이산적인 토큰들로 구성되어 있습니다. 원핫 인코딩은 이러한 이산적인 데이터를 컴퓨터가 처리하기 쉬운 연속적인 숫자로 변환하여 입력 데이터를 표현합니다.\n",
        "\n",
        "쉬운 연산: 원핫 인코딩된 벡터는 각 토큰이 고유한 인덱스 위치에만 1을 가지고 나머지는 0으로 표현되므로, 벡터 간의 내적과 덧셈 등의 연산이 간단합니다. 이러한 연산은 뉴럴 네트워크의 학습과 추론에서 매우 효율적으로 이루어질 수 있습니다.\n",
        "\n",
        "단어 간 유사성 비교: 원핫 인코딩은 단어 간 유사성을 계산하는데 사용할 수 있습니다. 유사한 단어들은 벡터 공간에서 더 가깝게 위치하게 되므로 단어 간 유사성을 계산하기가 용이합니다.\n",
        "\n",
        "단어를 원핫 인코딩하여 RNN에 입력으로 사용하면, 모델은 각 토큰을 독립적으로 처리하는 것이 아니라 시퀀스 상의 순서를 고려하여 문맥을 파악하게 됩니다. 이는 자연어 처리(NLP)와 같은 순차적인 데이터를 다루는데 매우 중요한 특성입니다."
      ],
      "metadata": {
        "id": "i-uaPg9bXcS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "x_train_onehot = to_categorical(x_train_seq)\n",
        "x_val_onehot = to_categorical(x_val_seq)\n",
        "\n",
        "print(x_train_onehot.shape)\n",
        "print(x_train_onehot.nbytes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrcmUSfBWK7R",
        "outputId": "e4394f62-58c3-41e9-bf35-de1fa3cbed44"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100, 100)\n",
            "800000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple rnn 클래스로 rnn 모델 만들기\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN"
      ],
      "metadata": {
        "id": "Utn5xJFzXzQA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SimpleRNN 매개변수 1 = 은닉층 개수 지정, 매개변수 2 = input_shape\n",
        "# Dense 매개변수 1 = 유닛개수, 매개변수 2 = 활성화 함수\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(SimpleRNN(32, input_shape=(100, 100)))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "2e6bSIfLYIjY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "#simple_rnn 계층 = 4256개의 파라미터\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm2jc0gEZtXK",
        "outputId": "e766d4dc-aab8-44f0-82fd-fb088f350323"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 32)                4256      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,289\n",
            "Trainable params: 4,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(x_train_onehot))\n",
        "print(type(y_train))\n",
        "print(type(x_val_onehot))\n",
        "print(type(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyQpimowbel-",
        "outputId": "8010157f-af19-4366-c53e-944863bead4d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_onehot.shape)\n",
        "print(x_val_onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxpDd318ck98",
        "outputId": "06dfc5a6-2c59-4cb7-b9e2-0028883be956"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100, 100)\n",
            "(5000, 100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  RNN 모델 훈련하기\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_onehot, y_train, epochs=20, batch_size=32, validation_data=(x_val_onehot, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "jQRZ5QUGZ01T",
        "outputId": "11e44e6d-5438-48a1-e8d1-4f5d2a34a4bf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-eefbd04a801d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실률은 커지고, accuracy와 val_acuuracy는 간의 격차가 커짐 -> 과대적합 가능성\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "id": "9MqEsPgvdhds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 임베딩층으로 RNN 학습시키기"
      ],
      "metadata": {
        "id": "jeXAiGuWjQNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩층은 자연어 처리(Natural Language Processing, NLP)에서 텍스트 데이터를 처리하기 위해 사용되는 중요한 요소 중 하나입니다. 임베딩층은 단어나 문장을 밀집된(dense) 공간으로 매핑하는 과정을 수행합니다. 이렇게 밀집된 표현은 희소(sparse)한 원핫 인코딩과는 달리 의미적 유사성을 고려하여 효율적으로 표현됩니다.\n",
        "\n",
        "임베딩층은 과대적합 문제를 직접적으로 해결하는 기법은 아닙니다. 과대적합 문제를 해결하기 위해서는 모델의 복잡도를 조정하거나 정규화 기법 등을 사용해야 합니다. 하지만 임베딩층을 사용하면 과대적합 문제를 완전히 해소하지는 못하지만, 과대적합에 대한 영향을 줄이고 모델의 성능을 개선하는데 도움이 됩니다.\n",
        "\n",
        "임베딩층이 과대적합 문제를 완전히 해소하지 못하는 이유는 임베딩층 자체가 모델에 파라미터로서 존재하기 때문입니다. 따라서 임베딩층은 모델이 훈련 데이터에 과도하게 적합되는 경우에도 영향을 미칩니다. 하지만 임베딩층은 단어들의 의미적 유사성을 반영하여 단어 간의 관계를 더 잘 이해하도록 도와주기 때문에 훈련 데이터에 대해 더 일반화된 표현을 학습할 수 있습니다."
      ],
      "metadata": {
        "id": "HnNlguuGe4J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩층으로 RNN 모델 훈련하기 => 단어 사이의 관계를 학습시킬 수 있음\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "(x_train_all, y_train_all), (x_test, y_test) = imdb.load_data(skip_top=20, num_words=1000)\n",
        "\n",
        "for i in range(len(x_train_all)):\n",
        "  x_train_all[i] = [w for w in x_train_all[i] if w > 2]\n",
        "\n",
        "x_train = x_train_all[random_index[:20000]]\n",
        "y_train = x_train_all[random_index[:20000]]\n",
        "\n",
        "x_val = x_train_all[random_index[20000:]]\n",
        "y_val = x_train_all[random_index[20000:]]"
      ],
      "metadata": {
        "id": "p7QcBmfMecxa"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100\n",
        "x_train_seq = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val_seq = sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "mEYmMveIiyp0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ebd = Sequential()\n",
        "\n",
        "model_ebd.add(Embedding(1000, 32))\n",
        "model_ebd.add(SimpleRNN(8))\n",
        "model_ebd.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_ebd.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJZK_WyAjMWh",
        "outputId": "2346f4c4-0ff1-4fda-f5e1-e52c92e2dfa6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          32000     \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 8)                 328       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,337\n",
            "Trainable params: 32,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ebd.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model_ebd.fit(x_train_seq, y_train, epochs=10, batch_size=32, validation_data=(x_val_seq, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "f4apISCujrii",
        "outputId": "ccadc3ba-eeb5-4bb4-ee71-bc0cafe7411e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-9ff288da1e90>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_ebd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ebd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "id": "YUuzRW3tkPb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model_ebd.evaluate(x_val_seq, y_val, verbose=0)\n",
        "print(loss, accuracy)"
      ],
      "metadata": {
        "id": "5IqUg0qckt2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}