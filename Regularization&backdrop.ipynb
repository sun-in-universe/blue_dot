{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6QXh2sY03bCa1EtO9YTk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sun-in-universe/blue_dot/blob/main/Regularization%26backdrop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규화\n",
        "= 과대적합 overfitting 문제 해결\n",
        "\n",
        "- 정규화 모드 : non-zero value에 lambd추가\n",
        "\"lambda\"를 지 않는 이유는 \"lambda\"가 python keyword이기 때문\n",
        "- drop out : 1보다 작은 keep_prop을 value에 setting하기\n",
        "\n",
        "우선 아무런 regularitiaon을 해본 후, 다음과 같은 정규화 실행\n",
        "\n",
        "- L2regularization functions : \"compute_cost_with_regularizaiton()\" & \"backward_proopagation_with_regularization()\"\n",
        "- Dropout functions : \"forward_propagation_with_dropout()\" & \"backward_propagation_with_dropout()\""
      ],
      "metadata": {
        "id": "rN_N20f2t-xL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0p7mbZDrvyo"
      },
      "outputs": [],
      "source": [
        "def model(X, Y, learning_rate=0.3, num_iterations = 30000, lamdb=0, keep_prop = 1)\n",
        "  grads = {}\n",
        "  costs = []\n",
        "  m = X.shape[1]\n",
        "  layer_dims = [X.shape[0], 20, 3, 1]\n",
        "\n",
        "  #initialize parameter dictionary\n",
        "  parameters = initialize_Xavier_parameter(layer_dims)\n",
        "\n",
        "  #Loop (gradient descent)\n",
        "\n",
        "  for i in range(0, num_iterations):\n",
        "    #forward propagation\n",
        "    if keep_prop == 1:\n",
        "      a3, cache = forward_propagation(X, parameters) #3번째 a값 a3, cache=W, b\n",
        "    #keep_prop < 1인 경우, 이 모델에서는 keep_prop이 1이기때문에 해당 X\n",
        "    elif keep_prop < 1:\n",
        "      a3, cache = forward_propagation_with_dropout(X, parameters, keep_prop)\n",
        "\n",
        "    #cost function\n",
        "    if lamdb = 0 :\n",
        "      cost = compute_cost(a3, Y)\n",
        "    #keep_prop와 마찬가지로 이 모델에서는 lamdb=0이기 때문에 해당 X, 아래 함수 구현\n",
        "    else:\n",
        "      cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
        "\n",
        "    #backward propagation\n",
        "    if lambd == 0 and keep_prob == 1:\n",
        "      grads = backward_propagation(X, Y, cache)\n",
        "    elif lambd != 0:\n",
        "      grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
        "    elif keep_prob < 1:\n",
        "      grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
        "\n",
        "    #update parameters.\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compute_cost_with_regularaization\n",
        "\n",
        "L2_regularization_cost = (1/𝑚) * (𝜆/2) * ∑𝑘∑𝑗𝑊[𝑙]2𝑘,𝑗\n",
        "\n",
        "cost = cross_entropy_cost + L2_regularization_cost\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "X3O-NdIWfnmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_cost_with_regularization\n",
        "\n",
        "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
        "  \"\"\"\n",
        "  implement the cost function with L2 regularization.\n",
        "\n",
        "  Arguments:\n",
        "  A3 -- post-activation, output of forward propagation\n",
        "  Y -- \"true\" labels vector, of shape\n",
        "  parameters -- python dictionary containing parameters of the model\n",
        "\n",
        "  Returns:\n",
        "  cost - value of the regularized loss function\n",
        "  \"\"\"\n",
        "\n",
        "  m = Y.shape[1]\n",
        "  W1 = parameters[\"W1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  W3 = parameters[\"W3\"]\n",
        "\n",
        "  cross_entropy_cost = compute_cost(A3, Y)\n",
        "\n",
        "  L2_regularization_cost = (1 / m) * (lambd / 2) * sum(np.sum(np.square(parameters[\"W\" + strl(l)])) for i in range(1, 4))\n",
        "  cost = cross_entropy_cost + L2_regularization_cost\n",
        "\n",
        "  return cost"
      ],
      "metadata": {
        "id": "3-RWiN5meWH7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##backward_propagation_with_regularization\n",
        "\n",
        "- 역전파 정규화를 하게 되면, dW1, dW2, dW3도 변화하게 된다.\n",
        "\n",
        "- 𝑑((1/2)* (𝜆/𝑚) * 𝑊**2)= (𝜆/𝑚) * 𝑊 ->각각의  gradients마다 더해주기"
      ],
      "metadata": {
        "id": "Y0SE-ktLh2Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#backward_propagation_with_regularization\n",
        "import numpy as np\n",
        "\n",
        "def backward_propagation_with_regularization(X,. Y, cache, lambd):\n",
        "  m = X.shape[1]\n",
        "  #cache를 파라미터에서 받았음에도 또 쓰는 이유는, 분해함으로써 역전파에서 사용 up\n",
        "  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "  dZ3 = A3 - Y\n",
        "  dW3 = (1/m) * np.dot(dZ3, A2.T) + (lambd / m) * W3\n",
        "  db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "  dA2 = np.dot(W3.T, dZ3)\n",
        "  #dA2와 np.int64를 곱한 이유는 relu함수 특성때문\n",
        "  dZ2 = np.multiply(dA2, np.int64(A2>0))\n",
        "  dW2 = (1/m) * np.dot(dZ2, A1.T) + (lambd / m) * W2\n",
        "  db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "  dA1= np.dot(W2.T, dZ2)\n",
        "  dZ1 = np.multiply(dA1, np.int64(A2 > 0))\n",
        "  dW1 = (1/m) * np.dot(dZ1, X.t) + (lambd) * W1\n",
        "  db1 = (1/m) * np.sum(dZ1, axis=1, keepdims= True)\n",
        "\n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
        "               \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n",
        "              \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "kAoPa7lgiePj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- lambd는 dev set에서 tuning할 수 있는 하이퍼파라미터다.\n",
        "- L2정규화는 결정 경계를 부드럽게 해준다 = 즉, 과대 적합을 방지한다. 하지만, 람다 값이 너무 크면 oversmooth해져서, high bias 문제가 발생할 수 있다.\n",
        "\n",
        "- L2 정규화 = 가중치를 작게 해줘서 모델을 심플하게 만들어준다. 즉, 손실함수에서 가중치의 제곱값에 벌점을줌으로써(penalty부여) 가중치를 줄일 수 있다.\n",
        "\n",
        "- cost computation = cost에 정규화 값을 더하는 것\n",
        "- backpropagation function = 가중치 행렬값의 미분값에 extra terms 추가\n",
        "- Weight decay\n",
        "= 가중치가 너무 작아져서 소멸되버리는것"
      ],
      "metadata": {
        "id": "yAImM0Giow_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dropout\n",
        "= 반복횟수 별로 몇몇 뉴런들을 shut down하는것\n",
        "\n",
        "- shout down = 0으로 만들어버리기\n",
        "- 1-keep_prob확률로 뉴런을 0으로 만든다.\n",
        "- dropped된 뉴런들은 순전파나 역전파에 기여하지 않는다.\n",
        "- 즉, dropout은 곧 각각의 반복횟수마다 서로 다른 모델을 훈련하는 것과 같다. 드롭아웃을 하게 되면, 뉴런이 덜 민감해지고 이는 곧 overfitting을 막는다!\n",
        "\n",
        "(1) Random matrix D1 = [d1, d2 ... ]을 A1과 같은 차원으로 만든다.\n",
        "(2) D1에서의 가각 요소들은 keep_prob이나 0을 가진다.\n",
        "(3) for i,v in enumerate(x):\n",
        "    if v < keep_prob:\n",
        "        x[i] = 1\n",
        "    else: # v >= keep_prob\n",
        "        x[i] = 0\n"
      ],
      "metadata": {
        "id": "SshOHIDEtYlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward_propagation_with_dropout\n",
        "\n",
        "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
        "  np. random.seed(1)\n",
        "\n",
        "  #retrive parameters\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "  W3 = parameters[\"W3\"]\n",
        "  b3 = parameters[\"b3\"]\n",
        "\n",
        "  Z1 = np.dot(W1, X) + b1\n",
        "  A1 = relu(Z1)\n",
        "\n",
        "  #step1 : initialize matrix D1\n",
        "  D1 = np.random.rand(A1.shape[0], A1.shape[1])\n",
        "  #step2 : covert entries of D1 to 0 or 1, astype(int): 이 부분은 불리언 값을 정수로 변환합니다. True는 1로, False는 0으로 변환됩니다.\n",
        "  D1 = (D1 < keep_prob).astype(int)\n",
        "  #step3 : shut down some neurons of A1\n",
        "  A1 = A1 * D1\n",
        "  #step4 : scale the value of neurons that haven't been shut down: 드롭아웃의 정규화 보장\n",
        "  A1 = A1 / (keep_prob)\n",
        "\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = relu(Z2)\n",
        "  D2 = np.random.rand(A2.shape[0], A2.shape[1])\n",
        "  D2 = (D2 < keep_prob).astype(int)\n",
        "  A2 = A2 * D2\n",
        "  A2 = A2 / (keep_prob)\n",
        "\n",
        "  Z3 = np.dot(W3, A2) + b3\n",
        "  A3 = sigmoid(Z3)\n",
        "\n",
        "  cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "\n",
        "  return A3, cache\n"
      ],
      "metadata": {
        "id": "dnY-3J84tWLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward_propagation_with_dropout\n",
        "\n",
        "(1) 같은 뉴런들을 shut down함\n",
        "(2) A1 / keep_prob했던 것 처럼 dA1 / keep_prop할 것임 for scaling"
      ],
      "metadata": {
        "id": "n-n8V8tuxiGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
        "\n",
        "  m = X.shape[1]\n",
        "  (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "  dZ3 = A3-Y\n",
        "  dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
        "  db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "  dA2 = np.dot(W3.T, dZ3)\n",
        "  #step1: apply mask D2 to shut down the same neurons\n",
        "  dA2 = dA2 * D2\n",
        "  #step2: scale the value of neurons that haven't been shout down\n",
        "  dA2 = dA2 / keep_prob\n",
        "\n",
        "  dZ2 = np.multiply(dA2, np.int64(A2>0))\n",
        "  dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "  db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dA1 = dA1 * D1\n",
        "  dA1 = dA1 / keep_prob\n",
        "\n",
        "  dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "  dW1 = (1/m) * np.dot(dZ1, X.T)\n",
        "  db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
        "               \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n",
        "                \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "Pp7rdU5cxhPK"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}