{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6QXh2sY03bCa1EtO9YTk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sun-in-universe/blue_dot/blob/main/Regularization%26backdrop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì •ê·œí™”\n",
        "= ê³¼ëŒ€ì í•© overfitting ë¬¸ì œ í•´ê²°\n",
        "\n",
        "- ì •ê·œí™” ëª¨ë“œ : non-zero valueì— lambdì¶”ê°€\n",
        "\"lambda\"ë¥¼ ì§€ ì•ŠëŠ” ì´ìœ ëŠ” \"lambda\"ê°€ python keywordì´ê¸° ë•Œë¬¸\n",
        "- drop out : 1ë³´ë‹¤ ì‘ì€ keep_propì„ valueì— settingí•˜ê¸°\n",
        "\n",
        "ìš°ì„  ì•„ë¬´ëŸ° regularitiaonì„ í•´ë³¸ í›„, ë‹¤ìŒê³¼ ê°™ì€ ì •ê·œí™” ì‹¤í–‰\n",
        "\n",
        "- L2regularization functions : \"compute_cost_with_regularizaiton()\" & \"backward_proopagation_with_regularization()\"\n",
        "- Dropout functions : \"forward_propagation_with_dropout()\" & \"backward_propagation_with_dropout()\""
      ],
      "metadata": {
        "id": "rN_N20f2t-xL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0p7mbZDrvyo"
      },
      "outputs": [],
      "source": [
        "def model(X, Y, learning_rate=0.3, num_iterations = 30000, lamdb=0, keep_prop = 1)\n",
        "  grads = {}\n",
        "  costs = []\n",
        "  m = X.shape[1]\n",
        "  layer_dims = [X.shape[0], 20, 3, 1]\n",
        "\n",
        "  #initialize parameter dictionary\n",
        "  parameters = initialize_Xavier_parameter(layer_dims)\n",
        "\n",
        "  #Loop (gradient descent)\n",
        "\n",
        "  for i in range(0, num_iterations):\n",
        "    #forward propagation\n",
        "    if keep_prop == 1:\n",
        "      a3, cache = forward_propagation(X, parameters) #3ë²ˆì§¸ aê°’ a3, cache=W, b\n",
        "    #keep_prop < 1ì¸ ê²½ìš°, ì´ ëª¨ë¸ì—ì„œëŠ” keep_propì´ 1ì´ê¸°ë•Œë¬¸ì— í•´ë‹¹ X\n",
        "    elif keep_prop < 1:\n",
        "      a3, cache = forward_propagation_with_dropout(X, parameters, keep_prop)\n",
        "\n",
        "    #cost function\n",
        "    if lamdb = 0 :\n",
        "      cost = compute_cost(a3, Y)\n",
        "    #keep_propì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ëª¨ë¸ì—ì„œëŠ” lamdb=0ì´ê¸° ë•Œë¬¸ì— í•´ë‹¹ X, ì•„ë˜ í•¨ìˆ˜ êµ¬í˜„\n",
        "    else:\n",
        "      cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
        "\n",
        "    #backward propagation\n",
        "    if lambd == 0 and keep_prob == 1:\n",
        "      grads = backward_propagation(X, Y, cache)\n",
        "    elif lambd != 0:\n",
        "      grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
        "    elif keep_prob < 1:\n",
        "      grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
        "\n",
        "    #update parameters.\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compute_cost_with_regularaization\n",
        "\n",
        "L2_regularization_cost = (1/ğ‘š) * (ğœ†/2) * âˆ‘ğ‘˜âˆ‘ğ‘—ğ‘Š[ğ‘™]2ğ‘˜,ğ‘—\n",
        "\n",
        "cost = cross_entropy_cost + L2_regularization_cost\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "X3O-NdIWfnmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_cost_with_regularization\n",
        "\n",
        "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
        "  \"\"\"\n",
        "  implement the cost function with L2 regularization.\n",
        "\n",
        "  Arguments:\n",
        "  A3 -- post-activation, output of forward propagation\n",
        "  Y -- \"true\" labels vector, of shape\n",
        "  parameters -- python dictionary containing parameters of the model\n",
        "\n",
        "  Returns:\n",
        "  cost - value of the regularized loss function\n",
        "  \"\"\"\n",
        "\n",
        "  m = Y.shape[1]\n",
        "  W1 = parameters[\"W1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  W3 = parameters[\"W3\"]\n",
        "\n",
        "  cross_entropy_cost = compute_cost(A3, Y)\n",
        "\n",
        "  L2_regularization_cost = (1 / m) * (lambd / 2) * sum(np.sum(np.square(parameters[\"W\" + strl(l)])) for i in range(1, 4))\n",
        "  cost = cross_entropy_cost + L2_regularization_cost\n",
        "\n",
        "  return cost"
      ],
      "metadata": {
        "id": "3-RWiN5meWH7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##backward_propagation_with_regularization\n",
        "\n",
        "- ì—­ì „íŒŒ ì •ê·œí™”ë¥¼ í•˜ê²Œ ë˜ë©´, dW1, dW2, dW3ë„ ë³€í™”í•˜ê²Œ ëœë‹¤.\n",
        "\n",
        "- ğ‘‘((1/2)* (ğœ†/ğ‘š) * ğ‘Š**2)= (ğœ†/ğ‘š) * ğ‘Š ->ê°ê°ì˜  gradientsë§ˆë‹¤ ë”í•´ì£¼ê¸°"
      ],
      "metadata": {
        "id": "Y0SE-ktLh2Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#backward_propagation_with_regularization\n",
        "import numpy as np\n",
        "\n",
        "def backward_propagation_with_regularization(X,. Y, cache, lambd):\n",
        "  m = X.shape[1]\n",
        "  #cacheë¥¼ íŒŒë¼ë¯¸í„°ì—ì„œ ë°›ì•˜ìŒì—ë„ ë˜ ì“°ëŠ” ì´ìœ ëŠ”, ë¶„í•´í•¨ìœ¼ë¡œì¨ ì—­ì „íŒŒì—ì„œ ì‚¬ìš© up\n",
        "  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "  dZ3 = A3 - Y\n",
        "  dW3 = (1/m) * np.dot(dZ3, A2.T) + (lambd / m) * W3\n",
        "  db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "  dA2 = np.dot(W3.T, dZ3)\n",
        "  #dA2ì™€ np.int64ë¥¼ ê³±í•œ ì´ìœ ëŠ” reluí•¨ìˆ˜ íŠ¹ì„±ë•Œë¬¸\n",
        "  dZ2 = np.multiply(dA2, np.int64(A2>0))\n",
        "  dW2 = (1/m) * np.dot(dZ2, A1.T) + (lambd / m) * W2\n",
        "  db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "  dA1= np.dot(W2.T, dZ2)\n",
        "  dZ1 = np.multiply(dA1, np.int64(A2 > 0))\n",
        "  dW1 = (1/m) * np.dot(dZ1, X.t) + (lambd) * W1\n",
        "  db1 = (1/m) * np.sum(dZ1, axis=1, keepdims= True)\n",
        "\n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
        "               \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n",
        "              \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "kAoPa7lgiePj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- lambdëŠ” dev setì—ì„œ tuningí•  ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë‹¤.\n",
        "- L2ì •ê·œí™”ëŠ” ê²°ì • ê²½ê³„ë¥¼ ë¶€ë“œëŸ½ê²Œ í•´ì¤€ë‹¤ = ì¦‰, ê³¼ëŒ€ ì í•©ì„ ë°©ì§€í•œë‹¤. í•˜ì§€ë§Œ, ëŒë‹¤ ê°’ì´ ë„ˆë¬´ í¬ë©´ oversmoothí•´ì ¸ì„œ, high bias ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "- L2 ì •ê·œí™” = ê°€ì¤‘ì¹˜ë¥¼ ì‘ê²Œ í•´ì¤˜ì„œ ëª¨ë¸ì„ ì‹¬í”Œí•˜ê²Œ ë§Œë“¤ì–´ì¤€ë‹¤. ì¦‰, ì†ì‹¤í•¨ìˆ˜ì—ì„œ ê°€ì¤‘ì¹˜ì˜ ì œê³±ê°’ì— ë²Œì ì„ì¤Œìœ¼ë¡œì¨(penaltyë¶€ì—¬) ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "- cost computation = costì— ì •ê·œí™” ê°’ì„ ë”í•˜ëŠ” ê²ƒ\n",
        "- backpropagation function = ê°€ì¤‘ì¹˜ í–‰ë ¬ê°’ì˜ ë¯¸ë¶„ê°’ì— extra terms ì¶”ê°€\n",
        "- Weight decay\n",
        "= ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì‘ì•„ì ¸ì„œ ì†Œë©¸ë˜ë²„ë¦¬ëŠ”ê²ƒ"
      ],
      "metadata": {
        "id": "yAImM0Giow_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dropout\n",
        "= ë°˜ë³µíšŸìˆ˜ ë³„ë¡œ ëª‡ëª‡ ë‰´ëŸ°ë“¤ì„ shut downí•˜ëŠ”ê²ƒ\n",
        "\n",
        "- shout down = 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦¬ê¸°\n",
        "- 1-keep_probí™•ë¥ ë¡œ ë‰´ëŸ°ì„ 0ìœ¼ë¡œ ë§Œë“ ë‹¤.\n",
        "- droppedëœ ë‰´ëŸ°ë“¤ì€ ìˆœì „íŒŒë‚˜ ì—­ì „íŒŒì— ê¸°ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "- ì¦‰, dropoutì€ ê³§ ê°ê°ì˜ ë°˜ë³µíšŸìˆ˜ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. ë“œë¡­ì•„ì›ƒì„ í•˜ê²Œ ë˜ë©´, ë‰´ëŸ°ì´ ëœ ë¯¼ê°í•´ì§€ê³  ì´ëŠ” ê³§ overfittingì„ ë§‰ëŠ”ë‹¤!\n",
        "\n",
        "(1) Random matrix D1 = [d1, d2 ... ]ì„ A1ê³¼ ê°™ì€ ì°¨ì›ìœ¼ë¡œ ë§Œë“ ë‹¤.\n",
        "(2) D1ì—ì„œì˜ ê°€ê° ìš”ì†Œë“¤ì€ keep_probì´ë‚˜ 0ì„ ê°€ì§„ë‹¤.\n",
        "(3) for i,v in enumerate(x):\n",
        "    if v < keep_prob:\n",
        "        x[i] = 1\n",
        "    else: # v >= keep_prob\n",
        "        x[i] = 0\n"
      ],
      "metadata": {
        "id": "SshOHIDEtYlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward_propagation_with_dropout\n",
        "\n",
        "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
        "  np. random.seed(1)\n",
        "\n",
        "  #retrive parameters\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "  W3 = parameters[\"W3\"]\n",
        "  b3 = parameters[\"b3\"]\n",
        "\n",
        "  Z1 = np.dot(W1, X) + b1\n",
        "  A1 = relu(Z1)\n",
        "\n",
        "  #step1 : initialize matrix D1\n",
        "  D1 = np.random.rand(A1.shape[0], A1.shape[1])\n",
        "  #step2 : covert entries of D1 to 0 or 1, astype(int): ì´ ë¶€ë¶„ì€ ë¶ˆë¦¬ì–¸ ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. TrueëŠ” 1ë¡œ, FalseëŠ” 0ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n",
        "  D1 = (D1 < keep_prob).astype(int)\n",
        "  #step3 : shut down some neurons of A1\n",
        "  A1 = A1 * D1\n",
        "  #step4 : scale the value of neurons that haven't been shut down: ë“œë¡­ì•„ì›ƒì˜ ì •ê·œí™” ë³´ì¥\n",
        "  A1 = A1 / (keep_prob)\n",
        "\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = relu(Z2)\n",
        "  D2 = np.random.rand(A2.shape[0], A2.shape[1])\n",
        "  D2 = (D2 < keep_prob).astype(int)\n",
        "  A2 = A2 * D2\n",
        "  A2 = A2 / (keep_prob)\n",
        "\n",
        "  Z3 = np.dot(W3, A2) + b3\n",
        "  A3 = sigmoid(Z3)\n",
        "\n",
        "  cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "\n",
        "  return A3, cache\n"
      ],
      "metadata": {
        "id": "dnY-3J84tWLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward_propagation_with_dropout\n",
        "\n",
        "(1) ê°™ì€ ë‰´ëŸ°ë“¤ì„ shut downí•¨\n",
        "(2) A1 / keep_probí–ˆë˜ ê²ƒ ì²˜ëŸ¼ dA1 / keep_propí•  ê²ƒì„ for scaling"
      ],
      "metadata": {
        "id": "n-n8V8tuxiGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
        "\n",
        "  m = X.shape[1]\n",
        "  (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "  dZ3 = A3-Y\n",
        "  dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
        "  db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "  dA2 = np.dot(W3.T, dZ3)\n",
        "  #step1: apply mask D2 to shut down the same neurons\n",
        "  dA2 = dA2 * D2\n",
        "  #step2: scale the value of neurons that haven't been shout down\n",
        "  dA2 = dA2 / keep_prob\n",
        "\n",
        "  dZ2 = np.multiply(dA2, np.int64(A2>0))\n",
        "  dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "  db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dA1 = dA1 * D1\n",
        "  dA1 = dA1 / keep_prob\n",
        "\n",
        "  dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "  dW1 = (1/m) * np.dot(dZ1, X.T)\n",
        "  db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
        "               \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n",
        "                \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "Pp7rdU5cxhPK"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}